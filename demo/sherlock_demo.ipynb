{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e85eb-f267-4583-a88a-b938e18072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# be sure to install all requirements in ../requirements.txt, including:\n",
    "! pip install jupyter_innotater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936ad7d-b5e2-4641-8e88-6b4ecb7d4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import json\n",
    "from jupyter_innotater import *\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4f68a-a5ce-454f-8e98-645cf45256fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2sherlock_urlandpath = {\n",
    "    'ViT-B/16': ('https://storage.googleapis.com/ai2-mosaic-public/projects/sherlock/pretrained_models/model%3DViT-B16~batch%3D512~warmup%3D500~lr%3D1e-05~valloss%3D0.0000~highlightbbox~widescreen_STEP%3D1800.pt', 'sherlock_vitb16.pt'),\n",
    "    'RN50x16': ('https://storage.googleapis.com/ai2-mosaic-public/projects/sherlock/pretrained_models/model%3DRN50x16~batch%3D200~warmup%3D500~lr%3D1e-05~valloss%3D0.0000~highlightbbox~widescreen_STEP%3D4500.pt', 'sherlock_rn50x16.pt'),\n",
    "    'RN50x64': ('https://storage.googleapis.com/ai2-mosaic-public/projects/sherlock/pretrained_models/model%3DRN50x64~batch%3D64~warmup%3D1000~lr%3D1e-05~valloss%3D0.0000~highlightbbox~widescreen_STEP%3D15600.pt', 'sherlock_rn50x64.pt'),\n",
    "    'RN50x64-multitask': ('https://storage.googleapis.com/ai2-mosaic-public/projects/sherlock/pretrained_models/model%3DRN50x64~batch%3D64~warmup%3D1000~lr%3D1e-05~valloss%3D0.0000~randomclueinfhighlightbbox~widescreen_STEP%3D25200.pt', 'sherlock_rn50x64_multitask.pt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c6917-0c63-4546-b5de-80bb61debe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## modify these parameters if you want!\n",
    "np.random.seed(1)\n",
    "clip_model = 'RN50x64-multitask' # you can change this to whichever in the above\n",
    "assert clip_model in model2sherlock_urlandpath\n",
    "batch_size_caption_features, workers_caption_features = 128, 4\n",
    "candidate_limit = 1000 # can be \"None\"\n",
    "data_directory = 'sherlock_demo_directory'\n",
    "image_path = 'coop_photo_small.jpg'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(os.getcwd())\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494fbb4-2aa2-491b-95a8-f06d1510a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the model + pretrained weights\n",
    "local_model_path = data_directory + '/' + model2sherlock_urlandpath[clip_model][1]\n",
    "\n",
    "model, preprocess = clip.load(clip_model.replace('-multitask',''), jit=False)\n",
    "try:\n",
    "    input_resolution = model.visual.input_resolution\n",
    "except:\n",
    "    input_resolution = model.input_resolution\n",
    "\n",
    "print('Getting model weights from {}'.format(model2sherlock_urlandpath[clip_model][0]))\n",
    "if not os.path.exists(local_model_path):\n",
    "    urllib.request.urlretrieve(model2sherlock_urlandpath[clip_model][0], local_model_path)\n",
    "\n",
    "state = torch.load(local_model_path, map_location=device)\n",
    "state['model_state_dict'] = {k.replace('module.clip_model.', '') : v for k, v in state['model_state_dict'].items()}\n",
    "state['model_state_dict'] = {k.replace('clip_model.', '') : v for k, v in state['model_state_dict'].items()}\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9970b-6847-458f-8a25-3443e1a2d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the validation clues/inferences to serve as candidates\n",
    "data_url = 'https://storage.googleapis.com/ai2-mosaic-public/projects/sherlock/data/sherlock_val_with_split_idxs_v1_1.json.zip'\n",
    "local_data_path = data_directory + '/sherlock_val_with_split_idxs_v1_1.json.zip'\n",
    "if not os.path.exists(local_data_path):\n",
    "    urllib.request.urlretrieve(data_url, local_data_path)\n",
    "        \n",
    "archive = zipfile.ZipFile(local_data_path, 'r')\n",
    "clues, inferences = [], []\n",
    "with archive.open('sherlock_val_with_split_idxs_v1_1.json',mode='r') as f:\n",
    "    val = json.load(f)\n",
    "\n",
    "all_clues = list(set([v['inputs']['clue'] for v in val]))\n",
    "all_inferences = list(set([v['targets']['inference'] for v in val]))\n",
    "np.random.shuffle(all_clues)\n",
    "np.random.shuffle(all_inferences)\n",
    "if candidate_limit:\n",
    "    all_clues = all_clues[:candidate_limit]\n",
    "    all_inferences = all_inferences[:candidate_limit]\n",
    "print('loaded {} clues and {} inferences'.format(len(all_clues), len(all_inferences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eec8b7-86fd-4352-ab39-810763a9d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper modeling code\n",
    "def clip_forward_image(model, image):\n",
    "    if len(image.shape) == 5:\n",
    "        im_feat1 = model.encode_image(image[:, 0, ...])\n",
    "        im_feat2 = model.encode_image(image[:, 1, ...])\n",
    "        image_features = (im_feat1 + im_feat2) / 2\n",
    "    else:\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    # normalized features                                                                                                                                                                                                                                                         \n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features\n",
    "\n",
    "\n",
    "def clip_forward_text(model, text):\n",
    "    text_features = model.encode_text(text)\n",
    "    # normalized features                                                                                                                                                                                                                                                         \n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82458eb6-c182-45b2-a438-a89e5e08135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the caption features\n",
    "class CLIPDatasetCaption(torch.utils.data.Dataset):\n",
    "    def __init__(self, captions, prefix=''):                                                                                                                                                                                                                               \n",
    "        self.captions = captions\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = clip.tokenize(self.prefix + '{}'.format(self.captions[idx]), truncate=True).squeeze()\n",
    "        return {'caption': caption}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "if '-multitask' in clip_model:\n",
    "    clue_prefix = 'clue: '\n",
    "    inference_prefix = 'inference: '\n",
    "else:\n",
    "    clue_prefix = inference_prefix = ''\n",
    "\n",
    "clue_iterator = torch.utils.data.DataLoader(\n",
    "        CLIPDatasetCaption(all_clues, clue_prefix),\n",
    "        batch_size=batch_size_caption_features, num_workers=workers_caption_features, shuffle=False)\n",
    "\n",
    "inference_iterator = torch.utils.data.DataLoader(\n",
    "        CLIPDatasetCaption(all_inferences, inference_prefix),\n",
    "        batch_size=batch_size_caption_features, num_workers=workers_caption_features, shuffle=False)\n",
    "\n",
    "all_clue_feats, all_inference_feats = [], []\n",
    "with torch.no_grad():\n",
    "    for c in tqdm.tqdm(clue_iterator):\n",
    "        all_clue_feats.append(clip_forward_text(model, c['caption'].to(device)).cpu().numpy())\n",
    "with torch.no_grad():\n",
    "    for c in tqdm.tqdm(inference_iterator):\n",
    "        all_inference_feats.append(clip_forward_text(model, c['caption'].to(device)).cpu().numpy())\n",
    "        \n",
    "all_clue_feats = np.vstack(all_clue_feats)\n",
    "all_inference_feats = np.vstack(all_inference_feats)\n",
    "clue_idx2clue = dict(enumerate(all_clues))\n",
    "inf_idx2inf = dict(enumerate(all_inferences))\n",
    "print('feature shape: {}, mapping length: {}'.format(all_clue_feats.shape, len(clue_idx2clue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839be8f-684e-4bd4-b885-2b01da5ab83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that highlight bboxes wrapped in a dataloader\n",
    "class ImageHighlightBboxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, resolution):\n",
    "        '''\n",
    "        images:\n",
    "            a list of [{'filepath': str, 'region': [ {'left': int, 'top': int, 'width': int, 'height':int } ]} ]\n",
    "        resolution:\n",
    "            the int resolution from CLIP\n",
    "        '''\n",
    "        self.images = images\n",
    "        self.preprocess = self._transform_test(resolution)\n",
    "    \n",
    "    @staticmethod\n",
    "    def highlight_region(image, bboxes):\n",
    "        image = image.convert('RGBA')\n",
    "        overlay = Image.new('RGBA', image.size, '#00000000')\n",
    "        draw = ImageDraw.Draw(overlay, 'RGBA')\n",
    "        for bbox in bboxes:\n",
    "            x = bbox['left']\n",
    "            y = bbox['top']\n",
    "            draw.rectangle([(x, y), (x+bbox['width'], y+bbox['height'])],\n",
    "                            fill='#ff05cd3c', outline='#05ff37ff', width=3)\n",
    "        \n",
    "        image = Image.alpha_composite(image, overlay)\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_test(n_px):\n",
    "        return Compose([\n",
    "            Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            lambda image: image.convert(\"RGB\"),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_to_torch_tensor(image, preprocess_fn):\n",
    "        width, height = image.size\n",
    "        if width >= height:\n",
    "            im1 = {'height': height, 'width': height, 'left': 0, 'top': 0}\n",
    "            im2 = {'height': height, 'width': height, 'left': width-height, 'top': 0}\n",
    "        else:\n",
    "            im1 = {'height': width, 'width': width, 'left': 0, 'top': 0}\n",
    "            im2 = {'height': width, 'width': width, 'left': 0, 'top': height-width}\n",
    "        regions = [image.crop((bbox['left'], bbox['top'], bbox['left'] + bbox['width'], bbox['top'] + bbox['height'])) for bbox in [im1, im2]]\n",
    "        image = torch.stack([preprocess_fn(r) for r in regions], 0)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.images[idx]\n",
    "        image = Image.open(c_data)\n",
    "        image = self.hide_region(image, c_data['bboxes'])\n",
    "        image = self.image_to_torch_tensor(image, self.preprocess)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c04f6b-959d-49a8-be43-06bb42b57418",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a bounding box\n",
    "targets = np.zeros((1, 4)) # Initialise bounding boxes as x,y = 0,0, width,height = 0,0\n",
    "Innotater( ImageInnotation([image_path]), BoundingBoxInnotation(targets) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612918fc-13e4-45d4-9529-bc86064c73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)\n",
    "image_with_highlight = ImageHighlightBboxDataset.highlight_region(\n",
    "    image, [{'left': targets[0, 0], 'top': targets[0, 1], 'width': targets[0, 2], 'height': targets[0, 3]}])\n",
    "transform = ImageHighlightBboxDataset._transform_test(input_resolution)\n",
    "image_as_tensor = ImageHighlightBboxDataset.image_to_torch_tensor(image_with_highlight, transform).unsqueeze(0)\n",
    "with torch.no_grad(): image_feature_vector = clip_forward_image(model, image_as_tensor).squeeze().cpu().numpy()\n",
    "image_feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb4c8c-f008-4d1c-b29f-c1022dfd9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clue_idxs = np.argsort(-image_feature_vector @ all_clue_feats.transpose())[:5]\n",
    "top_inf_idxs = np.argsort(-image_feature_vector @ all_inference_feats.transpose())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95bd46-3533-4508-8de3-2a9ead8f55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top clues (NOTE: THESE ARE UNRELIABLE EXCEPT FOR THE MULTITASK MODEL!):')\n",
    "for c_idx in top_clue_idxs:\n",
    "    print(clue_idx2clue[c_idx])\n",
    "print()\n",
    "print('Top inferences:')\n",
    "for c_idx in top_inf_idxs:\n",
    "    print(inf_idx2inf[c_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
